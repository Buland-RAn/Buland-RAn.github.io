<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>语言消歧 | AYu</title>
  <meta name="author" content="AYu">
  
  <meta name="description" content="学习生活片段记录">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="语言消歧"/>
  <meta property="og:site_name" content="AYu"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.ico" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="atom.xml" title="AYu" type="application/atom+xml">
</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">AYu</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archive" title="All the articles.">
			  <i class=""></i>Archive
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 语言消歧</h1>
		</div>
	


<script src='//unpkg.com/valine/dist/Valine.min.js'></script>

<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>在词义、句义、篇章含义层次都会出现语言根据上下文语义不同的现象，消歧即指根据上下文确定对象语义的过程。词义消歧即在词语层次上的语义消歧。语义消歧/词义消歧 是自然语言处理任务的一个核心与难点，影响了几乎所有任务的性能，比如搜索引擎、意见挖掘、文本理解与产生、推理等。</p>
<a id="more"></a>



<p>在语言学长期发展的过程中，语言本身积累了许多一词多义的用法。语言的产生是多方面共同作用的结果。语言的使用是不断变化的，一个词在发展中有许多具体的意思，现在通用的还有一些意思。不同地区可能对一个词有不同 的用法，不同的行业对一个词也会不同，甚至不同群体、不同个人、不同语气都会有自己的特殊的解读意思。语义消歧是一种语言理解的方式，一方面我们要理解通用词语一词多义的含义及应用，另一方面，还要考虑到具体场景，运用相关知识库、语料训练来增加一词多义的性能。</p>
<p>词性标注与词义消歧是相互关联的两个问题。二者都要依赖上下文来标注，但是词性标注比语义消歧要简单以及成功。原因主要是词性标注的标注集合是确定的，而语义消歧并没有，并且量级要大的多；词性标注的上下文依赖比语义消歧要短。</p>
<p>WSD应用</p>
<ul>
<li>机器翻译</li>
<li>信息检索 IR</li>
<li>文本挖掘与信息提取 IE</li>
<li>词典编纂</li>
</ul>
<h1 id="1-举例说明消歧在英文的tokenization工具中是如何实现的。"><a href="#1-举例说明消歧在英文的tokenization工具中是如何实现的。" class="headerlink" title="1. 举例说明消歧在英文的tokenization工具中是如何实现的。"></a>1. 举例说明消歧在英文的tokenization工具中是如何实现的。</h1><p>基本方法：</p>
<ul>
<li><p>基于字典的方法(如Lesk Algorithm)</p>
<ol>
<li><p>基于语义定义的消歧。如果词典中对W的 第i种定义 包含 词汇Ei，那么如果在一个包含W的句子中，同时也出现了Ei，那么就认为 在该句子中 W的语义应该取词典中的第i种定义。</p>
</li>
<li><p>基于类义辞典的消歧。词的每个语义 都定义其对应的主题或范畴（如“网球”对应的主题是“运动”），多个语义即对应了多个主题。如果W的上下文C中的词汇包含多个主题，则取其频率最高的主题，作为W的主题，确定了W的主题后，也就能确定其对应的语义。</p>
</li>
<li><p>基于双语对比的消歧。即把一种语言作为另一种语言的定义。例如，为了确定“interest”在英文句子A中的含义，可以利用句子A的中文表达，因为interest的不同语义在中文的表达是不同的。如果句子A对应中文包含“存款利率”，那么“interest”在句子A的语义就是“利率”。如果句子A的对应中文是“我对英语没有兴趣”，那么其语义就是“兴趣”。</p>
</li>
</ol>
</li>
<li><p>监督方法(如支持向量机、基于内存的学习、贝叶斯分类器、基于统计的学习)<br>利用带注释的语料库，上下文被认为是词的特征，而知识和推理被忽略</p>
</li>
<li><p>半监督方法<br>从种子数据进行引导</p>
</li>
<li><p>无监督方法<br>假设相似的含义发生在相似的上下文中。通过对上下文相似性的某种度量对词的含义进行聚类。</p>
</li>
<li><p>基于深度学习</p>
</li>
</ul>
<h2 id="1-1-NLTK-Lesk-Algorithm"><a href="#1-1-NLTK-Lesk-Algorithm" class="headerlink" title="1.1 NLTK(): Lesk Algorithm"></a>1.1 <strong>NLTK</strong>(): Lesk Algorithm</h2><p>单词的定义与当前上下文之间的度量重叠</p>
<blockquote>
<ol>
<li>It uses the definitions of the ambiguous word. Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.</li>
<li>Lesk’s algorithm disambiguates a target word by selecting the sense whose dictionary gloss shares the largest number of words with the glosses of neighboring words.</li>
</ol>
</blockquote>
<p>Context:</p>
<ul>
<li>It requires a <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Sense_Inventory">sense inventory</a> with <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Sense_Definition">word sense definitions</a>.</li>
<li>It compares the overlap between a <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Mention">word’s</a> <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Text_Window">text window</a> and the <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Mention">words</a> in the <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Sense_Definition">word sense definition</a>.</li>
<li>It selects the <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Sense_Definition">word sense</a> with the largest <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Set_Overlap_Measure">overlap</a>.</li>
<li>It can be extended by expanding the <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/Word_Mention">words</a> that will be compared. For example, by means of including similar words in <a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/WordNet">WordNet</a>. (<a target="_blank" rel="noopener" href="http://www.gabormelli.com/RKB/2002_AnAdaptedLeskAlgForWSDUsingWordNet">Banerjee &amp; Pedersen, 2002</a>)</li>
</ul>
<p>实现：</p>
<p><img src="/2021/introduction-to-WSD/image-20211021210504940.png" alt="image-20211021210504940"></p>
<p>source:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lesk</span>(<span class="params">context_sentence, ambiguous_word, pos=None, synsets=None</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a synset for an ambiguous word in a context.</span></span><br><span class="line"><span class="string">    :param iter context_sentence: The context sentence where the ambiguous word</span></span><br><span class="line"><span class="string">         occurs, passed as an iterable of words.</span></span><br><span class="line"><span class="string">    :param str ambiguous_word: The ambiguous word that requires WSD.</span></span><br><span class="line"><span class="string">    :param str pos: A specified Part-of-Speech (POS).</span></span><br><span class="line"><span class="string">    :param iter synsets: Possible synsets of the ambiguous word.</span></span><br><span class="line"><span class="string">    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    context = set(context_sentence)</span><br><span class="line">    <span class="keyword">if</span> synsets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        synsets = wordnet.synsets(ambiguous_word)</span><br><span class="line">    <span class="keyword">if</span> pos:</span><br><span class="line">        synsets = [ss <span class="keyword">for</span> ss <span class="keyword">in</span> synsets <span class="keyword">if</span> str(ss.pos()) == pos]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> synsets:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    _, sense = max(</span><br><span class="line">        (len(context.intersection(ss.definition().split())), ss) <span class="keyword">for</span> ss <span class="keyword">in</span> synsets</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> sense</span><br></pre></td></tr></table></figure>



<h2 id="1-2-pywsd中的三种语义消歧的实现"><a href="#1-2-pywsd中的三种语义消歧的实现" class="headerlink" title="1.2 pywsd中的三种语义消歧的实现"></a>1.2 <strong>pywsd</strong>中的三种语义消歧的实现</h2><ul>
<li><p><strong>Lesk algorithms</strong></p>
<ul>
<li>Original Lesk (Lesk, 1986)</li>
<li>Adapted/Extended Lesk (Banerjee and Pederson, 2002/2003)</li>
<li>Simple Lesk (with definition, example(s) and hyper+hyponyms)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_lesk</span>(<span class="params">context_sentence: str, ambiguous_word: str,</span></span></span><br><span class="line"><span class="function"><span class="params">                pos: str = None, lemma=True, stem=False, hyperhypo=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                stop=True, context_is_lemmatized=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                nbest=False, keepscore=False, normalizescore=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                from_cache=True</span>) -&gt; &quot;wn.Synset&quot;:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple Lesk is somewhere in between using more than the</span></span><br><span class="line"><span class="string">    original Lesk algorithm (1986) and using less signature</span></span><br><span class="line"><span class="string">    words than adapted Lesk (Banerjee and Pederson, 2002)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param context_sentence: String, sentence or document.</span></span><br><span class="line"><span class="string">    :param ambiguous_word: String, a single word.</span></span><br><span class="line"><span class="string">    :param pos: String, one of &#x27;a&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;n&#x27;, &#x27;v&#x27;, or None.</span></span><br><span class="line"><span class="string">    :return: A Synset for the estimated best sense.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Ensure that ambiguous word is a lemma.</span></span><br><span class="line">    ambiguous_word = lemmatize(ambiguous_word, pos=pos)</span><br><span class="line">    <span class="comment"># If ambiguous word not in WordNet return None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> wn.synsets(ambiguous_word):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># Get the signatures for each synset.</span></span><br><span class="line">    ss_sign = simple_signatures(ambiguous_word, pos, lemma, stem, hyperhypo, stop,</span><br><span class="line">                                from_cache=from_cache)</span><br><span class="line">    <span class="comment"># Disambiguate the sense in context.</span></span><br><span class="line">    context_sentence = context_sentence.split() <span class="keyword">if</span> context_is_lemmatized <span class="keyword">else</span> lemmatize_sentence(context_sentence)</span><br><span class="line">    <span class="keyword">return</span> compare_overlaps(context_sentence, ss_sign, nbest=nbest,</span><br><span class="line">                            keepscore=keepscore, normalizescore=normalizescore)</span><br></pre></td></tr></table></figure>

<ul>
<li>Cosine Lesk (use cosines to calculate overlaps instead of using raw counts)</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Maximizing Similarity</strong> (see also, <a target="_blank" rel="noopener" href="http://www.d.umn.edu/~tpederse/Pubs/max-sem-relate.pdf">Pedersen et al. (2003)</a>)</p>
<ul>
<li>Path similarity (Wu-Palmer, 1994; Leacock and Chodorow, 1998)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity_by_path</span>(<span class="params">sense1: <span class="string">&quot;wn.Synset&quot;</span>, sense2: <span class="string">&quot;wn.Synset&quot;</span>, option: str = <span class="string">&quot;path&quot;</span></span>) -&gt; float:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns maximum path similarity between two senses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param sense1: A synset.</span></span><br><span class="line"><span class="string">    :param sense2: A synset.</span></span><br><span class="line"><span class="string">    :param option: String, one of (&#x27;path&#x27;, &#x27;wup&#x27;, &#x27;lch&#x27;).</span></span><br><span class="line"><span class="string">    :return: A float, similarity measurement.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> option.lower() <span class="keyword">in</span> [<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path_similarity&quot;</span>]: <span class="comment"># Path similarities.</span></span><br><span class="line">        <span class="keyword">return</span> max(wn.path_similarity(sense1, sense2, if_none_return=<span class="number">0</span>),</span><br><span class="line">                   wn.path_similarity(sense2, sense1, if_none_return=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">elif</span> option.lower() <span class="keyword">in</span> [<span class="string">&quot;wup&quot;</span>, <span class="string">&quot;wupa&quot;</span>, <span class="string">&quot;wu-palmer&quot;</span>, <span class="string">&quot;wu-palmer&quot;</span>]: <span class="comment"># Wu-Palmer</span></span><br><span class="line">        <span class="keyword">return</span> max(wn.wup_similarity(sense1, sense2, if_none_return=<span class="number">0</span>),</span><br><span class="line">                   wn.wup_similarity(sense2, sense1, if_none_return=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">elif</span> option.lower() <span class="keyword">in</span> [<span class="string">&#x27;lch&#x27;</span>, <span class="string">&quot;leacock-chordorow&quot;</span>]: <span class="comment"># Leacock-Chodorow</span></span><br><span class="line">        <span class="keyword">if</span> sense1.pos != sense2.pos: <span class="comment"># lch can&#x27;t do diff POS</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> wn.lch_similarity(sense1, sense2, if_none_return=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Information Content (Resnik, 1995; Jiang and Corath, 1997; Lin, 1998)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_similarity</span>(<span class="params">context_sentence: str, ambiguous_word: str, option=<span class="string">&quot;path&quot;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   lemma=True, context_is_lemmatized=False, pos=None, best=True</span>) -&gt; &quot;wn.Synset&quot;:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform WSD by maximizing the sum of maximum similarity between possible</span></span><br><span class="line"><span class="string">    synsets of all words in the context sentence and the possible synsets of the</span></span><br><span class="line"><span class="string">    ambiguous words (see https://ibin.co/4gG9zUlejUUA.png):</span></span><br><span class="line"><span class="string">    &#123;argmax&#125;_&#123;synset(a)&#125;(\sum_&#123;i&#125;^&#123;n&#125;&#123;&#123;max&#125;_&#123;synset(i)&#125;(sim(i,a))&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param context_sentence: String, a sentence.</span></span><br><span class="line"><span class="string">    :param ambiguous_word: String, a single word.</span></span><br><span class="line"><span class="string">    :return: If best, returns only the best Synset, else returns a dict.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ambiguous_word = lemmatize(ambiguous_word)</span><br><span class="line">    <span class="comment"># If ambiguous word not in WordNet return None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> wn.synsets(ambiguous_word):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> context_is_lemmatized:</span><br><span class="line">        context_sentence = word_tokenize(context_sentence)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        context_sentence = [lemmatize(w) <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(context_sentence)]</span><br><span class="line">    result = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> wn.synsets(ambiguous_word, pos=pos):</span><br><span class="line">        result[i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> context_sentence:</span><br><span class="line">            _result = [<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> wn.synsets(j):</span><br><span class="line">                _result.append(sim(i,k,option))</span><br><span class="line">            result[i] += max(_result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> option <span class="keyword">in</span> [<span class="string">&quot;res&quot;</span>,<span class="string">&quot;resnik&quot;</span>]: <span class="comment"># lower score = more similar</span></span><br><span class="line">        result = sorted([(v,k) <span class="keyword">for</span> k,v <span class="keyword">in</span> result.items()])</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># higher score = more similar</span></span><br><span class="line">        result = sorted([(v,k) <span class="keyword">for</span> k,v <span class="keyword">in</span> result.items()],reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result[<span class="number">0</span>][<span class="number">1</span>] <span class="keyword">if</span> best <span class="keyword">else</span> result</span><br></pre></td></tr></table></figure>



<h1 id="2-从最长匹配到最大频率分词，体现了工程实践中的普遍规律。"><a href="#2-从最长匹配到最大频率分词，体现了工程实践中的普遍规律。" class="headerlink" title="2. 从最长匹配到最大频率分词，体现了工程实践中的普遍规律。"></a>2. 从最长匹配到最大频率分词，体现了工程实践中的普遍规律。</h1><p>规律：从完全的机械性的、工程性的问题转化为可以基于经验解决的问题。</p>
<p>从最长匹配到最大频率分词，分词任务已经从单纯的“算法”上升到了“建模”，即利用统计学方法结合大数据挖掘，对“语言”进行建模；从仅仅是<u>词</u>的层面上升到了<u>语义</u>的层面。</p>
<p>最长匹配分词中只需要一个完备的词典就可以完成分词任务，但这种切分结果与实际生活的关联并不太大；而最大频率分词使用到了统计方法，通过统计语料将人们日常生活中使用到的词、句子的形式的这个知识融入了模型中，去解决类似场景中的问题。个人理解最大频率分词更像是加入了先验概率（经验）得到的结果。</p>
<img src="/2021/introduction-to-WSD/image-20211020200133392.png" alt="image-20211020200133392" style="zoom:67%;">



<h1 id="3-站在工程技术角度，tokenization对于NLP的意义。"><a href="#3-站在工程技术角度，tokenization对于NLP的意义。" class="headerlink" title="3. 站在工程技术角度，tokenization对于NLP的意义。"></a>3. 站在工程技术角度，tokenization对于NLP的意义。</h1><blockquote>
<p>A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document. </p>
<p>This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning. They can also be used directly by a computer to trigger useful actions and responses. Or they might be used in a machine learning pipeline as features that trigger more complex decisions or behavior.</p>
</blockquote>
<ul>
<li>词是一个比较合适的粒度：通过分析词语的意思能够得到文本的意思，单字很多时候表达不了语义，而词语能够承载语义。</li>
<li>切断上下文耦合，降低词序的影响。让各组合之间的耦合性降低，以组合为特征，使其更接近我们的假设。</li>
<li>将复杂问题转化为数学问题：文本都是一些「非结构化数据」，我们需要先将这些数据转化为「结构化数据」，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。</li>
</ul>
<h1 id="4-证明最长匹配分词的合理性。"><a href="#4-证明最长匹配分词的合理性。" class="headerlink" title="4. 证明最长匹配分词的合理性。"></a>4. 证明最长匹配分词的合理性。</h1><p>intuition:</p>
<ol>
<li><p>最大匹配原则：使切分出来的词尽可能地少。</p>
</li>
<li><p>切分出来的词越多，表达的含义就越丰富，也会导致准确理解这句话的难度加大。</p>
</li>
<li><p>若在字典中存在，自然语言处理 与 自然/语言/处理 相比能简单直接表达意思。</p>
</li>
</ol>
<p>证明：</p>
<ul>
<li>首先，由于最长匹配分词是基于词典的方法，分出来的词不会出错。（不考虑歧义情况）</li>
<li>其次，最长匹配的算法也符合最大匹配原则，切分不会太细导致失去意义，且能给后续操作提供一个可能达到较高效率的分词序列。</li>
<li>最后，它能够通过在词典中添加新词来提高准确率，而不需要庞大的输入文本。</li>
</ul>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://neptune.ai/blog/tokenization-in-nlp">Tokenization in NLP – Types, Challenges, Examples, Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://www.wikiwand.com/zh-hans/%E8%AF%8D%E4%B9%89%E6%B6%88%E6%AD%A7">词义消歧 wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://www.wikiwand.com/en/Lesk_algorithm">Lesk algorithm wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-03-21-9">达观数据：综述中英文自然语言处理的异和同</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jclian91/p/10850704.html">词义消岐（WSD）的简介与实现</a></li>
<li><a target="_blank" rel="noopener" href="http://www.doc88.com/p-9959426974439.html">基于改进的 Lesk 算法的词义排歧算法.王永生</a></li>
<li><a href="./">WORD SENSE DISAMBIGUATION: A SURVEY.Alok Ranjan Pal and Diganta Saha</a></li>
<li><a target="_blank" rel="noopener" href="https://easyai.tech/ai-definition/tokenization/">分词 – Tokenization</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a101330107/article/details/78638146">中文分词综述</a></li>
<li><a target="_blank" rel="noopener" href="https://www.codenong.com/20896278/">关于nlp：Python中的词义消歧算法</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/alvations/pywsd">Python Implementations of Word Sense Disambiguation (WSD) Technologies.</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nltk.org/api/nltk.wsd.html">nltk.wsd module documentation</a></li>
<li><a target="_blank" rel="noopener" href="http://www.matrix67.com/blog/archives/4212">漫话中文自动分词和语义识别（上）：中文分词算法</a></li>
<li><a target="_blank" rel="noopener" href="https://www.6aiq.com/article/1551014488495">深度长文：中文分词的十年回顾</a></li>
<li><a target="_blank" rel="noopener" href="https://www.6aiq.com/article/1548171805699">中文分词技术及在 58 搜索的实践</a></li>
<li><a target="_blank" rel="noopener" href="https://www.6aiq.com/article/1606260024085">中文分词技术详解</a></li>
</ol>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2021/null/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> 上一页</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2021/http-proxy-in-cpp/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	<!-- 
<section id="comment">
    <h2 class="title">留言</h2>

    
</section>
 -->


    <section id="comment">
        <h2 class="title">
            留言
        </h2>
        <div id="vcomments"></div>
        <script>
            new Valine({
                el: '#vcomments',
                appId: '8FWjWdEoyYxwoFgc2ET6DcSh-gzGzoHsz',
                appKey: 'DqH3LDGq73YqdepEDCcPgWhl',
                visitor: true,
                // avatar: 'hide',
                avatar: 'retro',
                // 这里设置CDN, 默认微博表情CDN
                // emojiCDN: 'https://img.t.sinajs.cn/t4/appstyle/expression/ext/normal/',
                // // 表情title和图片映射
                // emojiMaps: {
                //     "smile": "e3/2018new_weixioa02_org.png",
                //     "lovely": "09/2018new_keai_org.png",
                //     // ... 更多表情
                // }
            })
        </script>
    </section>


    <style type="text/css">
        h2.title {
            font-size: 25px;
            margin-bottom: 27px;
        }

        .v[data-class=v] .vwrap {
            border: 3px solid #f0f0f0;
        }
    
        .v[data-class=v] .vwrap .vheader .vinput {
            border-bottom: 1.8px dashed #f5f5f5;
            color: #bebaba;
            font-size: 17px;
        }

        .v[data-class=v] .veditor {
            font-size: 1.2em;
            color: #b5b5b5;
        }

        .v[data-class=v] .vrow .vcol {
            
            font-size: 17px;
        }

        .v[data-class=v] .vicon {
            fill: #bebebe;
        }

        .v[data-class=v] .vrow {
            font-size: 0;
            padding: 10px 0 0;
        }

        .v[data-class=v] .vbtn {
            color: #d0cdcd;
        }
        
        .v[data-class=v] .vsys {
            padding: 0em 0em;
        }

        .v[data-class=v] .vcards .vcard {
            padding-top: 0em;
        }

        .v[data-class=v] p {
            margin-bottom: 0;
            color: #c0c0c0;
        }

        a:hover {
            font-weight: 800 !important;
        }

        .v[data-class=v] a:hover{
            color: #c66400;
            background: bottom !important;
        }

        .v[data-class=v] a.vnick:hover{
            color: #c66400 !important;
            background: bottom !important;
        }

        .v[data-class=v] .vwrap .vheader .vinput:focus {
            border-bottom-color: #ff7c29;
        }

    </style>





	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2021-10-30 
	</div>
	
	<span id="busuanzi_container_page_pv">
		<i class="fa fa-eye"></i>
  		<span id="busuanzi_value_page_pv"></span>
    </span>

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/NLP/">NLP<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/整理/">整理<span>21</span></a></li>
  
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
		   <span class="toc-title">Contents</span>
			<ol class="toc-article"><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#1-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E%E6%B6%88%E6%AD%A7%E5%9C%A8%E8%8B%B1%E6%96%87%E7%9A%84tokenization%E5%B7%A5%E5%85%B7%E4%B8%AD%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E3%80%82"><span class="toc-article-text">1. 举例说明消歧在英文的tokenization工具中是如何实现的。</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#1-1-NLTK-Lesk-Algorithm"><span class="toc-article-text">1.1 NLTK(): Lesk Algorithm</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#1-2-pywsd%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E8%AF%AD%E4%B9%89%E6%B6%88%E6%AD%A7%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-article-text">1.2 pywsd中的三种语义消歧的实现</span></a></li></ol></li><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#2-%E4%BB%8E%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D%E5%88%B0%E6%9C%80%E5%A4%A7%E9%A2%91%E7%8E%87%E5%88%86%E8%AF%8D%EF%BC%8C%E4%BD%93%E7%8E%B0%E4%BA%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E6%99%AE%E9%81%8D%E8%A7%84%E5%BE%8B%E3%80%82"><span class="toc-article-text">2. 从最长匹配到最大频率分词，体现了工程实践中的普遍规律。</span></a></li><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#3-%E7%AB%99%E5%9C%A8%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E8%A7%92%E5%BA%A6%EF%BC%8Ctokenization%E5%AF%B9%E4%BA%8ENLP%E7%9A%84%E6%84%8F%E4%B9%89%E3%80%82"><span class="toc-article-text">3. 站在工程技术角度，tokenization对于NLP的意义。</span></a></li><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#4-%E8%AF%81%E6%98%8E%E6%9C%80%E9%95%BF%E5%8C%B9%E9%85%8D%E5%88%86%E8%AF%8D%E7%9A%84%E5%90%88%E7%90%86%E6%80%A7%E3%80%82"><span class="toc-article-text">4. 证明最长匹配分词的合理性。</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99"><span class="toc-article-text">相关资料</span></a></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2022 AYu
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br>     
     <span id="busuanzi_container_site_pv">
    	👀: <span id="busuanzi_value_site_pv"></span>    |   🙋: <span id="busuanzi_value_site_uv"></span>
	</span>
</p>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
